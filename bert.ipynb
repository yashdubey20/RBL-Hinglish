{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-iHlr1sy_Ax"
      },
      "source": [
        "BERT Base MultiLingual cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9TGehPfy0wS"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers.utils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.utils'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load datasets\n",
        "bjp_data = pd.read_csv('BJP.csv', encoding='ISO-8859-1')\n",
        "congress_data = pd.read_csv('congress.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Combine datasets\n",
        "combined_data = pd.concat([bjp_data, congress_data], ignore_index=True)\n",
        "\n",
        "# Drop rows with NaN or non-string values in 'commentText'\n",
        "combined_data = combined_data.dropna(subset=['commentText'])\n",
        "combined_data = combined_data[combined_data['commentText'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize input texts\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "train_tokenized = tokenizer(list(train_data['commentText']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "val_tokenized = tokenizer(list(val_data['commentText']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_labels = torch.tensor(train_data['Label'].tolist())\n",
        "val_labels = torch.tensor(val_data['Label'].tolist())\n",
        "\n",
        "train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], train_labels)\n",
        "val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], val_labels)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss}, Validation Accuracy: {accuracy}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'bert_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWrMJ-5a2dDL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to plot confusion matrix with labels\n",
        "def plot_confusion_matrix(true_labels, predicted_labels):\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    labels = ['Negative', 'Positive']  # Assuming 0 is Negative and 1 is Positive\n",
        "\n",
        "    # Calculate TN, FN, FP, TP\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix\\nTN={}, FP={}, FN={}, TP={}'.format(tn, fp, fn, tp))\n",
        "    plt.show()\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plot_confusion_matrix(all_labels, all_preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB9PMD6Z3BDO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_losses, val_losses, and val_accuracies are lists containing the corresponding values for each epoch\n",
        "train_losses = [0.033, 0.035, 0.025, 0.017, 0.028]  # Example values, replace with actual data\n",
        "val_losses = [0.675, 0.829, 0.815, 0.824, 0.70]   # Example values, replace with actual data\n",
        "val_accuracies = [0.81, 0.83, 0.84, 0.82, 0.84]  # Example values, replace with actual data\n",
        "'''\n",
        "Epoch 1/5, Loss: 100.22155168652534, Validation Accuracy: 0.8184143222506394\n",
        "Epoch 2/5, Loss: 62.78327962011099, Validation Accuracy: 0.8363171355498721\n",
        "Epoch 3/5, Loss: 37.99849247466773, Validation Accuracy: 0.8465473145780051\n",
        "Epoch 4/5, Loss: 30.744250578805804, Validation Accuracy: 0.8260869565217391\n",
        "Epoch 5/5, Loss: 16.05345555371605, Validation Accuracy: 0.8465473145780051\n",
        "Epoch 1/5, Average Training Loss: 0.03302575466763561, Average Validation Loss: 0.675539287966581\n",
        "Epoch 2/5, Average Training Loss: 0.035095677736312704, Average Validation Loss: 0.8297827266577195\n",
        "Epoch 3/5, Average Training Loss: 0.02527293006501075, Average Validation Loss: 0.815096401910022\n",
        "Epoch 4/5, Average Training Loss: 0.01761852964157196, Average Validation Loss: 0.8244812604472307\n",
        "Epoch 5/5, Average Training Loss: 0.028966635979096673, Average Validation Loss: 0.7092047963167863\n",
        "'''\n",
        "\n",
        "def plot_learning_curve(train_losses, val_losses, val_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the learning curve\n",
        "plot_learning_curve(train_losses, val_losses, val_accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10zZbvBbzHoX"
      },
      "source": [
        "Code for Input-Output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEfpTTCXzLnZ"
      },
      "outputs": [],
      "source": [
        "# Function to classify input text\n",
        "def classify_text(text):\n",
        "    # Tokenize input text\n",
        "    tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_text['input_ids'].to(device)\n",
        "    attention_mask = tokenized_text['attention_mask'].to(device)\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = output.logits\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Map predicted label to sentiment\n",
        "    sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "    return sentiment\n",
        "\n",
        "# Example usage\n",
        "input_text = \"modi ne mst kaam kiya hai!\"\n",
        "output_sentiment = classify_text(input_text)\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Predicted Sentiment:\", output_sentiment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFL_wXhszOgg"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbsrabAWzQeC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
